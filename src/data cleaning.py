# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vPlXEu_OlNN3Dj0NFGOh91i_UZF4xbbv
"""

# Libraries

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder
from sklearn.utils.validation import check_is_fitted

RANDOM_STATE = 42

# Utility: quick audit prints

def audit_df(df, name="df", n_head=5):
    print(f"\n=== {name} audit ===")
    print("Shape:", df.shape)
    print("Columns:", list(df.columns))
    print("\nDtypes:\n", df.dtypes)
    # Missingness
    na = df.isna().sum()
    if na.sum() == 0:
        print("\nMissing values: none")
    else:
        print("\nMissing values:")
        print(na[na > 0].sort_values(ascending=False).head(20))
    print("\nHead:\n", df.head(n_head))

# Load data (adjust paths if needed)

credit_path = "creditcard.csv"
paysim_path = "frauddetection.csv"

credit_df = pd.read_csv(credit_path)
paysim_df = pd.read_csv(paysim_path)

audit_df(credit_df, "CreditCard (raw)")
audit_df(paysim_df, "PaySim (raw)")

# CREDIT CARD: cleaning
# Columns: Time, V1 - V28, Amount, Class

# 1) Remove duplicate rows if any
credit_df = credit_df.drop_duplicates()

# 2) Ensure numeric types (coerce non-numeric to NaN, then handle)
numeric_cols_cc = ["Time", "Amount"] + [f"V{i}" for i in range(1, 29)]
for c in numeric_cols_cc + ["Class"]:
    if c in credit_df.columns:
        credit_df[c] = pd.to_numeric(credit_df[c], errors="coerce")

# 3) Handle missing values (rare in the canonical dataset). Here:
#  drop rows with NaN in any model feature or target.
credit_df = credit_df.dropna(subset=numeric_cols_cc + ["Class"])

# 4) Separate features & target
X_cc = credit_df[numeric_cols_cc].copy()
y_cc = credit_df["Class"].astype(int).copy()

# CREDIT CARD: preprocessing pipeline
# - Standard scale Time + V1..V28
# - Robust scale Amount due to heavy-tailed distribution
cc_numeric_standard = ["Time"] + [f"V{i}" for i in range(1, 29)]
cc_numeric_robust = ["Amount"]

preprocessor_cc = ColumnTransformer(
    transformers=[
        ("std", StandardScaler(), cc_numeric_standard),
        ("rob", RobustScaler(with_centering=True, with_scaling=True), cc_numeric_robust),
    ],
    remainder="drop",
    verbose_feature_names_out=False
)

cc_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor_cc)
])

X_cc_processed = cc_pipeline.fit_transform(X_cc)

# Collect feature names after transform
cc_feature_names = cc_pipeline.named_steps["preprocessor"].get_feature_names_out()

print("\n=== CreditCard processed ===")
print("X_cc_processed shape:", X_cc_processed.shape)
print("y_cc shape:", y_cc.shape)
print("First 5 feature names:", cc_feature_names[:5])

# PAYSIM: cleaning
# Columns (canonical): step, type, amount, nameOrig, oldbalanceOrg,
# newbalanceOrig, nameDest, oldbalanceDest, newbalanceDest, isFraud, isFlaggedFraud
paysim_df = paysim_df.drop_duplicates()

# Ensure expected dtypes
num_cols_ps = [
    "step", "amount",
    "oldbalanceOrg", "newbalanceOrig",
    "oldbalanceDest", "newbalanceDest",
    "isFraud", "isFlaggedFraud"
]
for c in num_cols_ps:
    if c in paysim_df.columns:
        paysim_df[c] = pd.to_numeric(paysim_df[c], errors="coerce")

# Handle missing values minimally:
# - If NaNs exist in essential numeric columns, drop those rows
essential_ps = ["step", "type", "amount", "oldbalanceOrg", "newbalanceOrig",
                "oldbalanceDest", "newbalanceDest", "isFraud"]
paysim_df = paysim_df.dropna(subset=[c for c in essential_ps if c in paysim_df.columns])

# Engineer leakage-resistant residual features
# The PaySim data sometimes makes raw balances near function of amount and target.
# Residuals help reduce direct leakage.
paysim_df["delta_orig"] = paysim_df["oldbalanceOrg"] - paysim_df["amount"] - paysim_df["newbalanceOrig"]
paysim_df["delta_dest"] = paysim_df["newbalanceDest"] - paysim_df["oldbalanceDest"] - paysim_df["amount"]

# Simple error flags (negative deltas can signify inconsistent sequences)
paysim_df["error_orig"] = (paysim_df["delta_orig"].abs() < 1e-9).astype(int)  # exact consistency marker
paysim_df["error_dest"] = (paysim_df["delta_dest"].abs() < 1e-9).astype(int)

# Drop IDs & raw balances to limit leakage
drop_cols_ps = ["nameOrig", "nameDest", "oldbalanceOrg", "newbalanceOrg", "oldbalanceDest", "newbalanceDest"]
for c in drop_cols_ps:
    if c in paysim_df.columns:
        paysim_df = paysim_df.drop(columns=c)

# Final feature set split
target_ps = "isFraud"
X_ps = paysim_df.drop(columns=[target_ps])
y_ps = paysim_df[target_ps].astype(int)

# Identify column types
cat_cols_ps = ["type"] if "type" in X_ps.columns else []
num_cols_ps_final = [c for c in X_ps.columns if c not in cat_cols_ps]

# PAYSIM: preprocessing pipeline
# - OneHot encode 'type' (unknown categories safe)
# - Robust scale numeric features (handles large outliers)
preprocessor_ps = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols_ps),
        ("num", RobustScaler(with_centering=True, with_scaling=True), num_cols_ps_final),
    ],
    remainder="drop",
    verbose_feature_names_out=False
)

ps_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor_ps)
])

X_ps_processed = ps_pipeline.fit_transform(X_ps)

# Collect feature names
# (cat feature names + numeric names)
enc_cat = ps_pipeline.named_steps["preprocessor"].named_transformers_.get("cat", None)
if enc_cat is not None and len(cat_cols_ps) > 0:
    cat_feature_names = enc_cat.get_feature_names_out(cat_cols_ps)
else:
    cat_feature_names = np.array([])

num_feature_names = np.array(num_cols_ps_final)
ps_feature_names = np.concatenate([cat_feature_names, num_feature_names])

print("\n=== PaySim processed ===")
print("X_ps_processed shape:", X_ps_processed.shape)
print("y_ps shape:", y_ps.shape)
print("First 10 feature names:", ps_feature_names[:10])

# Save cleaned data & processed features

credit_df.to_csv("clean_creditcard.csv", index=False)
pd.DataFrame(X_cc_processed, columns=cc_feature_names).to_csv("credit_features_processed.csv", index=False)
pd.DataFrame({"Class": y_cc}).to_csv("credit_target.csv", index=False)

paysim_df.to_csv("clean_paysim.csv", index=False)
pd.DataFrame(X_ps_processed, columns=ps_feature_names).to_csv("paysim_features_processed.csv", index=False)
pd.DataFrame({"isFraud": y_ps}).to_csv("paysim_target.csv", index=False)