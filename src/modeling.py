# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vPlXEu_OlNN3Dj0NFGOh91i_UZF4xbbv
"""

# Convergence-safe Analytical ML (Colab-friendly)
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    roc_auc_score, average_precision_score, f1_score, matthews_corrcoef,
    confusion_matrix, precision_recall_curve
)

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Helpers
def stratified_70_15_15(X, y, random_state=RANDOM_STATE):
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.30, stratify=y, random_state=random_state
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=random_state
    )
    return (X_train, y_train, X_val, y_val, X_test, y_test)

def evaluate_scores(y_true, y_score, threshold=0.5):
    y_pred = (y_score >= threshold).astype(int)
    return {
        "ROC_AUC": roc_auc_score(y_true, y_score),
        "PR_AUC":  average_precision_score(y_true, y_score),
        "F1":      f1_score(y_true, y_pred, zero_division=0),
        "MCC":     matthews_corrcoef(y_true, y_pred),
        "threshold": float(threshold),
        "TP": int(((y_true==1) & (y_pred==1)).sum()),
        "FP": int(((y_true==0) & (y_pred==1)).sum()),
        "TN": int(((y_true==0) & (y_pred==0)).sum()),
        "FN": int(((y_true==1) & (y_pred==0)).sum()),
    }

def search_threshold(y_true, y_score, mode="F1"):
    precision, recall, ths = precision_recall_curve(y_true, y_score)
    ths = np.append(ths, 1.0)
    best = {"score": -np.inf, "threshold": 0.5}
    for t in ths:
        y_pred = (y_score >= t).astype(int)
        score = matthews_corrcoef(y_true, y_pred) if mode.upper()=="MCC" else f1_score(y_true, y_pred, zero_division=0)
        if score > best["score"]:
            best = {"score": float(score), "threshold": float(t)}
    return best

def recall_at_k(y_true, y_score, k=0.01):
    n = len(y_true)
    top = max(1, int(np.ceil(k * n)))
    idx = np.argsort(-y_score)[:top]
    positives = (y_true == 1).sum()
    return float((y_true[idx] == 1).sum()) / max(1, positives)

def to_float32(*arrays):
    return [np.asarray(a, dtype=np.float32) for a in arrays]

# Convert engineered matrices to float32 (reduces RAM, improves speed)
X_cc_tr = np.asarray(X_cc_tr, dtype=np.float32)
y_cc    = np.asarray(y_cc,    dtype=np.int32)

X_ps_tr = np.asarray(X_ps_tr, dtype=np.float32)
y_ps    = np.asarray(y_ps,    dtype=np.int32)

MAX_PS = 500_000  # adjust down if RAM is tight
if X_ps_tr.shape[0] > MAX_PS:
    # stratified random sample
    from sklearn.model_selection import StratifiedShuffleSplit
    sss = StratifiedShuffleSplit(n_splits=1, test_size=(X_ps_tr.shape[0]-MAX_PS)/X_ps_tr.shape[0], random_state=RANDOM_STATE)
    for keep_idx, _ in sss.split(X_ps_tr, y_ps):
        X_ps_tr = X_ps_tr[keep_idx]
        y_ps    = y_ps[keep_idx]
    print(f"[Info] PaySim subsampled to: {X_ps_tr.shape}")

# Split
cc_train = stratified_70_15_15(X_cc_tr, y_cc)
ps_train = stratified_70_15_15(X_ps_tr, y_ps)

# LOGISTIC REGRESSION: use saga + more iters + tighter tol
# Docs:
# - Preprocessing scaling: https://scikit-learn.org/stable/modules/preprocessing.html
# - Solver options & convergence: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression

def run_logreg_safe(train_tuple, C=1.0, penalty="l2", solver="saga",
                    max_iter=3000, tol=1e-4, class_weight="balanced", calibrate=False):
    X_train, y_train, X_val, y_val, X_test, y_test = train_tuple

    # Model settings aimed at convergence & memory stability:
    # - solver= saga handles large/sparse, supports L1/L2
    # - max_iter increased substantially
    # - tol slightly smaller for accuracy, but not too small to stall
    # - class_weight='balanced' for imbalance without resampling
    lr = LogisticRegression(
        C=C, penalty=penalty, solver=solver, max_iter=max_iter, tol=tol,
        class_weight=class_weight, random_state=RANDOM_STATE, n_jobs=1  # keep n_jobs low to avoid RAM spikes
    )

    lr.fit(X_train, y_train)

    val_score  = lr.predict_proba(X_val)[:, 1]
    test_score = lr.predict_proba(X_test)[:, 1]

    best_f1  = search_threshold(y_val, val_score, mode="F1")
    best_mcc = search_threshold(y_val, val_score, mode="MCC")

    res_f1  = evaluate_scores(y_test, test_score, threshold=best_f1["threshold"])
    res_mcc = evaluate_scores(y_test, test_score, threshold=best_mcc["threshold"])

    return lr, {
        "val_best_F1": best_f1,
        "val_best_MCC": best_mcc,
        "test_at_valF1": res_f1,
        "test_at_valMCC": res_mcc,
        "val_recall@1%": recall_at_k(y_val, val_score, k=0.01),
        "test_recall@1%": recall_at_k(y_test, test_score, k=0.01),
        "val_scores": val_score,
        "test_scores": test_score,
    }

# ALTERNATIVE: SGDClassifier (log loss) for very large data
# - Performs stochastic optimization; usually converges fast.
# - Good fallback if LR still complains.
def run_sgd_logit(train_tuple, alpha=1e-4, max_iter=20, tol=1e-3):
    X_train, y_train, X_val, y_val, X_test, y_test = train_tuple
    sgd = SGDClassifier(
        loss="log_loss", penalty="l2", alpha=alpha, max_iter=max_iter, tol=tol,
        class_weight="balanced", random_state=RANDOM_STATE
    )
    sgd.fit(X_train, y_train)
    # SGD doesn't have predict_proba by default; use decision_function -> sigmoid
    from scipy.special import expit
    val_score  = expit(sgd.decision_function(X_val))
    test_score = expit(sgd.decision_function(X_test))
    best_f1  = search_threshold(y_val, val_score, mode="F1")
    best_mcc = search_threshold(y_val, val_score, mode="MCC")
    res_f1  = evaluate_scores(y_test, test_score, threshold=best_f1["threshold"])
    res_mcc = evaluate_scores(y_test, test_score, threshold=best_mcc["threshold"])
    return sgd, {
        "val_best_F1": best_f1,
        "val_best_MCC": best_mcc,
        "test_at_valF1": res_f1,
        "test_at_valMCC": res_mcc,
        "val_recall@1%": recall_at_k(y_val, val_score, k=0.01),
        "test_recall@1%": recall_at_k(y_test, test_score, k=0.01),
        "val_scores": val_score,
        "test_scores": test_score,
    }


# RANDOM FOREST: keep moderate to avoid RAM spikes

def run_rf_safe(train_tuple, n_estimators=200, max_depth=None, min_samples_leaf=2):
    X_train, y_train, X_val, y_val, X_test, y_test = train_tuple
    rf = RandomForestClassifier(
        n_estimators=n_estimators, max_depth=max_depth, min_samples_leaf=min_samples_leaf,
        class_weight="balanced", random_state=RANDOM_STATE, n_jobs=-1
    )
    rf.fit(X_train, y_train)
    val_score  = rf.predict_proba(X_val)[:, 1]
    test_score = rf.predict_proba(X_test)[:, 1]
    best_f1  = search_threshold(y_val, val_score, mode="F1")
    best_mcc = search_threshold(y_val, val_score, mode="MCC")
    res_f1  = evaluate_scores(y_test, test_score, threshold=best_f1["threshold"])
    res_mcc = evaluate_scores(y_test, test_score, threshold=best_mcc["threshold"])
    return rf, {
        "val_best_F1": best_f1,
        "val_best_MCC": best_mcc,
        "test_at_valF1": res_f1,
        "test_at_valMCC": res_mcc,
        "val_recall@1%": recall_at_k(y_val, val_score, k=0.01),
        "test_recall@1%": recall_at_k(y_test, test_score, k=0.01),
        "val_scores": val_score,
        "test_scores": test_score,
    }

# Run CREDIT CARD
print("\n######## CREDIT CARD ########")
cc_lr, cc_lr_res = run_logreg_safe(cc_train, C=1.0, penalty="l2", solver="saga", max_iter=3000, tol=1e-4)
print("CC | LogisticRegression:", cc_lr_res)

# cc_sgd, cc_sgd_res = run_sgd_logit(cc_train, alpha=1e-4, max_iter=20, tol=1e-3)
# print("CC | SGD (logit):", cc_sgd_res)

cc_rf, cc_rf_res = run_rf_safe(cc_train, n_estimators=200, max_depth=None, min_samples_leaf=2)
print("CC | RandomForest:", cc_rf_res)

# Run PAYSIM
print("\n######## PAYSIM ########")
ps_lr, ps_lr_res = run_logreg_safe(ps_train, C=1.0, penalty="l2", solver="saga", max_iter=3000, tol=1e-4)
print("PS | LogisticRegression:", ps_lr_res)

# Fallback to SGD only if needed

ps_rf, ps_rf_res = run_rf_safe(ps_train, n_estimators=200, max_depth=None, min_samples_leaf=2)
print("PS | RandomForest:", ps_rf_res)